{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação de bibliotecas e sessão do pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import functools\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando esquemas para as tabelas:\n",
    "* Definindo o cabeçalho e os tipos de dados de cada coluna na tabela de clientes.\n",
    "* Lendo arquivos CSVs a partir do esquema definido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "    .add('Id', IntegerType(),True) \\\n",
    "    .add('Nome', StringType(),True) \\\n",
    "    .add('Email', StringType(),True) \\\n",
    "    .add('Data_cadastro', TimestampType(),True) \\\n",
    "    .add('Telefone', StringType(),True)\n",
    "\n",
    "df_clients = spark.read.format('csv') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .schema(schema) \\\n",
    "    .load('./data/clients/', delimiter=';')\n",
    "\n",
    "df_clients.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Definindo o cabeçalho e os tipos de dados de cada coluna na tabela de Transações.\n",
    "* Lendo arquivos CSVs a partir do esquema definido.\n",
    "* Adicionando valor negativo às transações de saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema2 = StructType() \\\n",
    "    .add('Id', IntegerType(),True) \\\n",
    "    .add('Cliente_Id', IntegerType(),True) \\\n",
    "    .add('Valor', FloatType(),True) \\\n",
    "    .add('DataHora', TimestampType(),True)\n",
    "\n",
    "df_transaction_in = spark.read.format('csv') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .schema(schema2) \\\n",
    "    .load('./data/transaction/in/', delimiter=';')\n",
    "    \n",
    "df_transaction_out = spark.read.format('csv') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .schema(schema2) \\\n",
    "    .load('./data/transaction/out/', delimiter=';')\n",
    "\n",
    "df_transaction_out.withColumn('Valor', - df_transaction_out['Valor'])\n",
    "df_transaction_out.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unindo dataframes de transações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(dfs):\n",
    "    return functools.reduce(\n",
    "        lambda df_transaction_in, \n",
    "        df_transaction_out: \n",
    "            df_transaction_in.union(df_transaction_out.select(df_transaction_in.columns)), dfs)\n",
    "\n",
    "df_transaction = unionAll([df_transaction_in, df_transaction_out])\n",
    "df_transaction.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização dos dados:\n",
    "\n",
    "* Tranformar todos os dados em lower case. \n",
    "* Limpar espaços em branco desnecessários.\n",
    "* Retirar qualquer linha com dados que não se encaixam no esquema definido (cabeçalhos lidos como dados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clients = df_clients.withColumn('Nome', F.lower(df_clients['Nome']))\n",
    "\n",
    "df_clients = df_clients.withColumn('Nome', F.trim(df_clients.Nome))\n",
    "\n",
    "df_clients = df_clients.na.drop('any')\n",
    "\n",
    "df_transaction = df_transaction.na.drop('all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adicionando colunas:\n",
    "\n",
    "* Criando coluna 'DDD' e 'Country_code' a partir dos dados da coluna 'Telefone'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clients = df_clients \\\n",
    "    .withColumn('DDD', F.substring('Telefone', 5, 2)) \\\n",
    "    .withColumn('Country_code', F.substring('Telefone', 1, 3))\n",
    "\n",
    "df_clients.show()\n",
    "print(df_clients.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Criando colunas 'Hora', 'Minuto', 'Segundo' e 'Dia' a partir dos dados da coluna 'DataHora'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction = df_transaction \\\n",
    "    .withColumn('Hora', F.hour(F.col('DataHora'))) \\\n",
    "    .withColumn('Minuto', F.minute(F.col('DataHora'))) \\\n",
    "    .withColumn('Segundo', F.second(F.col('DataHora'))) \\\n",
    "    .withColumn('Dia', F.to_date(F.col('DataHora'))) \\\n",
    "\n",
    "df_transaction.show()\n",
    "print(df_transaction.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar csv com dados tratados:\n",
    "* Salvar em único csv todos os dados de clientes tratados na pasta 'clients_clean':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clients.coalesce(1).write.option('header','true') \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv('./data/clients/clients_clean')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Salvar em único csv todos os dados de transações tratados na pasta 'transaction_clean':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction.coalesce(1).write.option('header','true') \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv('./data/transaction/transaction_clean')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentativa de popular tabelas no SQL server via pyspark:\n",
    "* Não foi concluída pois não encontramos o drive específico para tal tarefa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# server = os.environ['SERVER']\n",
    "# database = os.environ['DATABASE']\n",
    "# username = os.environ['USERNAME']\n",
    "# password = os.environ['PASSWORD']\n",
    "\n",
    "# df_clients.write \\\n",
    "#   .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "#   .mode(\"append\") \\\n",
    "#   .option(\"url\", \"jdbc:sqlserver://anti-fraude.database.windows.net:1433;databaseName=Anti_fraude_s_a;\") \\\n",
    "#   .option(\"dbtable\", \"clientes\") \\\n",
    "#   .option(\"user\", username) \\\n",
    "#   .option(\"password\", password) \\\n",
    "#   .option(\"encrypt\", True) \\\n",
    "#   .option(\"trustServerCertificate\", False) \\\n",
    "#   .option(\"hostNameInCertificate\", '*database.windows.net') \\\n",
    "#   .option(\"loginTimeout\", 30) \\\n",
    "#   .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50c2d20383134e315f3c8f7279e89f10e9ffbad1cf5063fc5b9085ee4d294136"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
